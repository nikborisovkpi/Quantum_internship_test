{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c946d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\Quantum1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification, pipeline\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb06a4",
   "metadata": {},
   "source": [
    "### NER_model train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba70140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, label2id, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        word_labels = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text.split(),\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=False\n",
    "        )\n",
    "\n",
    "        labels = []\n",
    "        word_ids = encoding.word_ids() # use word_ids to align labels\n",
    "        previous_word_id = None\n",
    "\n",
    "        # Word labeling logic\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                label = word_labels[word_id]\n",
    "                if label == \"O\":\n",
    "                    labels.append(self.label2id[label])\n",
    "                else:\n",
    "                    # If token is part of same word, put I-MOUNTAIN \n",
    "                    if word_id != previous_word_id:\n",
    "                        labels.append(self.label2id[\"B-MOUNTAIN\"])\n",
    "                    else:\n",
    "                        labels.append(self.label2id[\"I-MOUNTAIN\"])\n",
    "                previous_word_id = word_id\n",
    "\n",
    "        encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        encoding[\"labels\"] = torch.tensor(labels)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f3ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_data(file_path):\n",
    "    texts, labels_list = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text, labels = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # Пустая строка означает конец текста\n",
    "                if text and labels:\n",
    "                    texts.append(\" \".join(text))\n",
    "                    labels_list.append(labels)\n",
    "                text, labels = [], []\n",
    "            else:\n",
    "                word, label = line.split()\n",
    "                text.append(word)\n",
    "                labels.append(label)\n",
    "    return texts, labels_list\n",
    "\n",
    "\n",
    "\n",
    "# Загрузка данных из файла\n",
    "train_texts, train_labels = load_synthetic_data('train_data.txt')\n",
    "val_texts, val_labels = load_synthetic_data('val_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9136fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fuji is visible from hundreds of kilometers away.',\n",
       "  ['B-MOUNTAIN', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
       " ('I wrote my name in the summit register of Elbrus.',\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNTAIN']),\n",
       " ('The wind at the top of K2 was over 100 km/h.',\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNTAIN', 'O', 'O', 'O', 'O']),\n",
       " ('K2 has multiple named routes of varying difficulty.',\n",
       "  ['B-MOUNTAIN', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
       " ('The wind at the top of Everest was over 100 km/h.',\n",
       "  ['O', 'O', 'O', 'O', 'O', 'O', 'B-MOUNTAIN', 'O', 'O', 'O', 'O'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(train_texts, train_labels))\n",
    "val_data = list(zip(val_texts, val_labels))\n",
    "\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3b3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Compute main metrics\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels, pred_labels = [], []\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        for p, l in zip(pred, lab):\n",
    "            if l != -100:\n",
    "                true_labels.append(l)\n",
    "                pred_labels.append(p)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average='weighted', zero_division=0\n",
    "    )\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c66e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model_train(train_path=\"train_data.txt\", val_path=\"val_data.txt\", output_dir=\"./ner_model\", epochs=3, batch_size=16):\n",
    "    # Загрузка данных из файла\n",
    "    train_texts, train_labels = load_synthetic_data(train_path)\n",
    "    val_texts, val_labels = load_synthetic_data(val_path)\n",
    "\n",
    "    # Put labels\n",
    "    label_list = [\"O\", \"B-MOUNTAIN\", \"I-MOUNTAIN\"]\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    # Load tokkenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=len(label_list),\n",
    "        id2label={i: l for i, l in enumerate(label_list)},\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NERDataset(train_texts, train_labels, tokenizer, label2id)\n",
    "    val_dataset = NERDataset(val_texts, val_labels, tokenizer, label2id)\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        eval_strategy=\"epoch\",             # metrics after every epoch\n",
    "        save_strategy=\"no\",                # do not save checkpoints\n",
    "        logging_strategy=\"epoch\",           \n",
    "        logging_dir=\"./logs\",               \n",
    "        load_best_model_at_end=False,\n",
    "        report_to=\"none\",                   \n",
    "    )\n",
    "\n",
    "    # Trainer with metrics\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Model training \n",
    "    trainer.train()\n",
    "\n",
    "    # Model saving\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb7be161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nikita Borisov\\AppData\\Local\\Temp\\ipykernel_8012\\1688228340.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 01:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.997330</td>\n",
       "      <td>0.997321</td>\n",
       "      <td>0.997323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_model_train(train_path=\"train_data.txt\", val_path=\"val_data.txt\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f88a3",
   "metadata": {},
   "source": [
    "### NER_model inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b329e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mountains(model_dir, text):\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "    # Create ner pipeline\n",
    "    nlp = pipeline(\n",
    "        \"ner\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        aggregation_strategy=\"simple\"  # create word from B/I tokens\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    results = nlp(text)\n",
    "    \n",
    "    # Take labels\n",
    "    mountains = [r[\"word\"].strip(',.!?') for r in results if \"MOUNTAIN\" in r[\"entity_group\"]]\n",
    "    return mountains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29716265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_dir, text):\n",
    "    mountains = extract_mountains(model_dir, text)\n",
    "    if mountains:\n",
    "        print(f'Mountains found: {\", \".join(mountain.capitalize() for mountain in mountains)}')\n",
    "    else: \n",
    "        print(\"Mountains not found!\")\n",
    "    # print(mountains[0].capitalize() if mountains else \"Mountains not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44572b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mountains found: Everest, K2\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_dir=\"./ner_model\", \n",
    "    text=\"The Everest is the highest mountain. The K2 is also very tall.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
